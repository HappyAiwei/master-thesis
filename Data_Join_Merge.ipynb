{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geoplot as gplt\n",
    "import shapefile\n",
    "import osr\n",
    "import dbf\n",
    "import requests\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countypairs = pd.read_csv('/home/jinli/PycharmProjects/county-pair-list.txt')\n",
    "countypairs.drop_duplicates(subset='COUNTYPAIR_ID', inplace = True)\n",
    "\n",
    "new = countypairs['COUNTYPAIR_ID'].str.split(\"-\", n = 1, expand = True)\n",
    "\n",
    "pairid = pd.concat([new[0], new[1]], ignore_index=True)\n",
    "pairid = pairid.drop_duplicates()\n",
    "pairid = pairid.to_frame('id')\n",
    "pairid = pairid[~pairid['id'].isin(['06001', '06041', '06081','06075'])] ### county fips changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAUS data (Unemployment Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/home/jinli/Desktop/Thesis/Data/LAU(unemployment_rate)/*.csv')\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "lausdata = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "lausdata['GEOID10'] = lausdata['Series ID'].map(lambda x: x[5:10])\n",
    "lausdata['STATEFP10'] = lausdata['GEOID10'].map(lambda x: x[0:2])\n",
    "lausdata['COUNTYFP10'] = lausdata['GEOID10'].map(lambda x: x[2:])\n",
    "\n",
    "lausdata = pd.merge(pairid, lausdata, how='left', left_on='id', right_on='GEOID10')\n",
    "lausdata = lausdata.dropna()\n",
    "### a = lausdata.GEOID10.unique()\n",
    "### len(a)      1126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lausdata.to_csv('LAUS_COUNTY_MONTHLY_UNEMPLOYMENT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform monthly LAUS data to quarterly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlaus = pd.read_csv('LAUS_COUNTY_MONTHLY_UNEMPLOYMENT.csv')\n",
    "\n",
    "mlaus['id'] = mlaus['id'].astype(str)\n",
    "mlaus['id'] = mlaus['id'].str.zfill(5)\n",
    "\n",
    "mlaus['Year'] = mlaus['Year'].astype(int)\n",
    "mlaus = mlaus[(mlaus['Year'] > 2005) & (mlaus['Year'] < 2016)]\n",
    "\n",
    "mlaus['Value'] = pd.to_numeric(mlaus['Value'], errors='coerce')\n",
    "mlaus = mlaus.replace(np.nan, 0, regex=True)\n",
    "\n",
    "mlaus['Month'] = mlaus['Period'].str[1:]\n",
    "mlaus['Year-Month'] = pd.to_datetime(mlaus[['Year', 'Month']].assign(Day=1)).dt.to_period('M')\n",
    "mlaus['Qtr'] = pd.to_datetime(mlaus[['Year', 'Month']].assign(Day=1)).dt.quarter\n",
    "\n",
    "mlaus = mlaus[['id', 'Year', 'Qtr', 'Value']]\n",
    "mlaus.sort_values(by =['id', 'Year', 'Qtr'], ignore_index=True, inplace=True)\n",
    "\n",
    "qlaus = mlaus.groupby(['id', 'Year', 'Qtr'])['Value'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlaus.to_csv('LAUS_COUNTY_QUARTERLY_UNEMPLOYMENT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoPlot: all states, all counties and broder counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All counties\n",
    "allcounties = ZipFile('/home/jinli/PycharmProjects/tl_2010_us_county10(NEW).zip', 'r')\n",
    "\n",
    "filenames_ac = [y for y in sorted(allcounties.namelist())\n",
    "                 for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)]\n",
    "\n",
    "dbf_ac, prj_ac, shp_ac, shx_ac = [io.BytesIO(allcounties.read(filename)) for filename in filenames_ac]\n",
    "\n",
    "r_ac = shapefile.Reader(shp=shp_ac, shx=shx_ac, dbf=dbf_ac)\n",
    "## r_ac.numRecords   ### 3221\n",
    "\n",
    "\n",
    "attributes, geometry = [], []\n",
    "\n",
    "field_names = [field[0] for field in r_ac.fields[1:]]\n",
    "\n",
    "for row in r_ac.shapeRecords():\n",
    "    geometry.append(shape(row.shape.__geo_interface__))\n",
    "    attributes.append(dict(zip(field_names,row.record)))\n",
    "    \n",
    "prj = io.TextIOWrapper(prj_ac, encoding='utf-8')\n",
    "proj4 = osr.SpatialReference(prj.read()).ExportToProj4()\n",
    "\n",
    "gdf_ac = gpd.GeoDataFrame(data=attributes, geometry=geometry, crs=proj4)\n",
    "gdf_ac.sort_values(by =['STATEFP10', 'COUNTYFP10'], inplace=True)\n",
    "gdf_ac.reset_index(drop=True, inplace=True)\n",
    "gdf_ac[['INTPTLON10', 'INTPTLAT10']] = gdf_ac[['INTPTLON10', 'INTPTLAT10']].apply(pd.to_numeric)\n",
    "\n",
    "gdf_ac = gdf_ac[(gdf_ac.STATEFP10 != '02') & (gdf_ac.STATEFP10 != '72') & (gdf_ac.STATEFP10 != '15')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All states\n",
    "allstates = ZipFile('/home/jinli/PycharmProjects/tl_2010_us_state10.zip', 'r')\n",
    "\n",
    "filenames_as = [y for y in sorted(allstates.namelist())\n",
    "                for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)]\n",
    "\n",
    "dbf_as, prj_as, shp_as, shx_as = [io.BytesIO(allstates.read(filename)) for filename in filenames_as]\n",
    "\n",
    "r_as = shapefile.Reader(shp=shp_as, shx=shx_as, dbf=dbf_as)\n",
    "\n",
    "attributes, geometry = [], []\n",
    "\n",
    "field_names = [field[0] for field in r_as.fields[1:]]\n",
    "for row in r_as.shapeRecords():\n",
    "    geometry.append(shape(row.shape.__geo_interface__))\n",
    "    attributes.append(dict(zip(field_names,row.record)))\n",
    "    \n",
    "prj = io.TextIOWrapper(prj_as, encoding='utf-8')\n",
    "proj4 = osr.SpatialReference(prj.read()).ExportToProj4()\n",
    "\n",
    "gdf_as = gpd.GeoDataFrame(data=attributes, geometry=geometry, crs=proj4)\n",
    "gdf_as = gdf_as[~gdf_as['STATEFP10'].isin(['02', '72', '15'])]\n",
    "gdf_as.sort_values(by ='STATEFP10', ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_list = lausdata.GEOID10.tolist()\n",
    "cp_list = list(set(cp_list)) ## remove duplicates\n",
    "#cp_list = [e for e in cp_list if e not in ('06001', '06041', '06081','06075')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cp = gdf_ac[gdf_ac['GEOID10'].isin(cp_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "gdf_cp.plot(ax=ax, color='steelblue', edgecolor='none')\n",
    "gdf_ac.plot(ax=ax, facecolor='none', linewidth=0.2, edgecolor='grey')\n",
    "gdf_as.plot(ax=ax, facecolor='none', linewidth=1, edgecolor='black')\n",
    "\n",
    "fig.savefig('full_figure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCEW (Quarterly Census of Employment and Wages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('Paired_County_ID.csv')\n",
    "ids['id'] = ids['id'].astype(str)\n",
    "ids['id'] = ids['id'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/home/jinli/Desktop/Thesis/Data/QCEW/2005.q1-q4.by_area/*.csv')\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "qcewdata = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcewdata05 = qcewdata[qcewdata['area_fips'].apply(lambda x: str(x).isdigit())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcewdata05['area_fips'] = qcewdata05['area_fips'].astype(str)\n",
    "qcewdata05['area_fips'] = qcewdata05['area_fips'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcewdata05 = pd.merge(qcewdata05, ids, how='left', left_on='area_fips', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01001 = pd.read_csv('/home/jinli/Desktop/Thesis/Data/QCEW/2005.q1-q4.by_area/2005.q1-q4 01001 Autauga County, Alabama.csv')\n",
    "df01001['area_fips'] = df01001['area_fips'].astype(str)\n",
    "df01001['area_fips'] = df01001['area_fips'].str.zfill(5)\n",
    "df01001.drop(df01001.columns[21:], axis=1, inplace=True)\n",
    "df01001 = df01001.loc[df01001['own_code']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Benefit Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbe = pd.read_excel('MaxBenefitExtension.xlsx', index_col=0) \n",
    "\n",
    "mbe.drop(columns=['2016Q1', '2016Q2', '2016Q3', '2016Q4'], inplace=True)\n",
    "mbe.reset_index(inplace=True)\n",
    "mbe = mbe.iloc[:, 2:]\n",
    "\n",
    "mbe = mbe.melt(id_vars=['state_id'], ignore_index=True)\n",
    "mbe.fillna(0, inplace=True) \n",
    "mbe['value'] += 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbe.to_csv('Maximum_Benefit_Duration.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:  county-pair ids and centroid distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_dist = pd.read_csv('CountyPair_Centroid_Border_Distance.csv') ### 1178 entries\n",
    "cp_id = pd.read_csv('Paired_County_ID.csv') ### 1126 entries\n",
    "\n",
    "cp_dist = pd.merge(cp_dist, cp_id, how='left', left_on='GEOID10_FIPS1', right_on='id')  \n",
    "cp_dist = pd.merge(cp_dist, cp_id, how='left', left_on='GEOID10_FIPS2', right_on='id')\n",
    "#7   id_x            1166 non-null   float64\n",
    "#8   id_y            1172 non-null   float64\n",
    "cp_dist.dropna(inplace=True)  ### 1163 rows × 9 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: cp_dist and benefit extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbe = pd.read_csv('Maximum_Benefit_Duration.csv')\n",
    "\n",
    "df = pd.merge(cp_dist, mbe, how='outer', left_on='STATE_FIPS1', right_on='state_id') \n",
    "# cp_dist.shape (1163, 9)\n",
    "# mbe.shape (2120, 3)   53*40=2120\n",
    "# df.shape (46680, 12)  1163*40=46520\n",
    "#---  ------          --------------  -----\n",
    "# 1   STATE_FIPS1     46520 non-null  float64\n",
    "# 9   state_id        46680 non-null  int64\n",
    "df.dropna(inplace=True)\n",
    "df.drop(df.columns[7:10], axis=1, inplace=True)\n",
    "df.rename({'variable': 'Period', 'value': 'BenefitDuration1'}, axis=1, inplace=True)\n",
    "df = df[['COUNTYPAIR_ID', 'Period', \n",
    "         'STATE_FIPS1', 'BenefitDuration1', 'GEOID10_FIPS1', 'distance_FIPS1', \n",
    "         'STATE_FIPS2', 'GEOID10_FIPS2', 'distance_FIPS2']]\n",
    "\n",
    "\n",
    "df = pd.merge(df, mbe, left_on=['STATE_FIPS2', 'Period'], right_on=['state_id', 'variable'])\n",
    "df.drop(['variable'], axis=1, inplace=True)\n",
    "df.rename({'value': 'BenefitDuration2'}, axis=1, inplace=True)\n",
    "df = df[['COUNTYPAIR_ID', 'Period', \n",
    "         'STATE_FIPS1', 'BenefitDuration1', 'GEOID10_FIPS1', 'distance_FIPS1', \n",
    "         'STATE_FIPS2', 'BenefitDuration2', 'GEOID10_FIPS2', 'distance_FIPS2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: merging quarterly unemployment rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ur = pd.read_csv('LAUS_COUNTY_QUARTERLY_UNEMPLOYMENT.csv')\n",
    "\n",
    "ur['Period'] = ur.Year.map(str) + 'Q' + ur.Qtr.map(str)\n",
    "ur = ur[['id', 'Period', 'Value']]\n",
    "ur.rename({'Value': 'UR'}, axis=1, inplace=True)\n",
    "\n",
    "df1 = df[['COUNTYPAIR_ID', 'Period', 'STATE_FIPS1', 'BenefitDuration1', 'GEOID10_FIPS1', 'distance_FIPS1']]\n",
    "df2 = df[['COUNTYPAIR_ID', 'Period', 'STATE_FIPS2', 'BenefitDuration2', 'GEOID10_FIPS2', 'distance_FIPS2']]\n",
    "\n",
    "df1 = pd.merge(df1, ur, left_on=['GEOID10_FIPS1', 'Period'], right_on=['id', 'Period'])\n",
    "df2 = pd.merge(df2, ur, left_on=['GEOID10_FIPS2', 'Period'], right_on=['id', 'Period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: merging QCEW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcew = pd.read_csv('QCEW_QUARTERLY_CENSUS_of_EMPLOYMENT_and_WAGES.csv')\n",
    "\n",
    "qcew['Period'] = qcew.year.map(str) + 'Q' + qcew.qtr.map(str)\n",
    "\n",
    "df1 = pd.merge(df1, qcew, left_on=['GEOID10_FIPS1', 'Period'], right_on=['area_fips', 'Period'])\n",
    "df2 = pd.merge(df2, qcew, left_on=['GEOID10_FIPS2', 'Period'], right_on=['area_fips', 'Period'])\n",
    "\n",
    "df1.drop(['id_x', 'area_fips', 'id_y'], axis=1, inplace=True)\n",
    "df1_1 = df1.iloc[:, 0:6]\n",
    "df1_2 = df1.iloc[:, 6:]\n",
    "df1_2 = df1_2.add_suffix('_1')\n",
    "df1 = pd.concat([df1_1, df1_2], axis=1)\n",
    "\n",
    "df2.drop(['id_x', 'area_fips', 'id_y'], axis=1, inplace=True)\n",
    "df2_1 = df2.iloc[:, 0:6]\n",
    "df2_2 = df2.iloc[:, 6:]\n",
    "df2_2 = df2_2.add_suffix('_2')\n",
    "df2 = pd.concat([df2_1, df2_2], axis=1)\n",
    "\n",
    "df1.sort_values(by =['COUNTYPAIR_ID', 'Period'], inplace=True)\n",
    "df2.sort_values(by =['COUNTYPAIR_ID', 'Period'], inplace=True)\n",
    "\n",
    "#df1.to_csv('DataFrame1.csv', index=False)\n",
    "#df2.to_csv('DataFrame2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: drop duplicates to generate pannel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dd = df1.drop(['COUNTYPAIR_ID'], axis=1)\n",
    "\n",
    "first_col = ['GEOID10_FIPS1']\n",
    "last_cols = [col for col in df1_dd.columns if col not in first_col]\n",
    "\n",
    "df1_dd = df1_dd[first_col + last_cols]\n",
    "\n",
    "df1_dd = df1_dd.drop_duplicates(subset=['GEOID10_FIPS1', 'Period'])\n",
    "\n",
    "df1_dd.to_csv('DataFrame1_Drop_Duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_dd = df2.drop(['COUNTYPAIR_ID'], axis=1)\n",
    "\n",
    "first_col = ['GEOID10_FIPS2']\n",
    "last_cols = [col for col in df2_dd.columns if col not in first_col]\n",
    "\n",
    "df2_dd = df2_dd[first_col + last_cols]\n",
    "\n",
    "df2_dd = df2_dd.drop_duplicates(subset=['GEOID10_FIPS2', 'Period'])\n",
    "\n",
    "df2_dd.to_csv('DataFrame2_Drop_Duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: generate county-pair-data with 1 lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read total separation rates\n",
    "tsr = pd.read_csv('TotalSeparationRates_Nonfarm_QuarterlySum .csv')\n",
    "tsr['time'] = pd.PeriodIndex(tsr.DATE, freq='Q')\n",
    "tsr = tsr[['DATE', 'time', 'JTSTSR']]\n",
    "# period[Q-DEC] column to string\n",
    "tsr.time = tsr.time.astype(str)\n",
    "tsr['time'] = tsr['time'].str.replace('Q', 'q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinli/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('DataFrame1.csv')\n",
    "df2 = pd.read_csv('DataFrame2.csv')\n",
    "\n",
    "df1_dd_lead1 = pd.read_csv('DataFrame1_Drop_Duplicates_Lead1.csv')\n",
    "df2_dd_lead1 = pd.read_csv('DataFrame2_Drop_Duplicates_Lead1.csv')\n",
    "\n",
    "df1_dd_lead1 = pd.merge(df1_dd_lead1, tsr, on='time', validate='many_to_one')\n",
    "df2_dd_lead1 = pd.merge(df2_dd_lead1, tsr, on='time', validate='many_to_one')\n",
    "\n",
    "df1_dd_lead1.drop(df1_dd_lead1.iloc[:, 6:28], axis=1, inplace=True)\n",
    "df2_dd_lead1.drop(df2_dd_lead1.iloc[:, 6:28], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31680 entries, 0 to 31679\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   geoid10_fips2     31680 non-null  int64  \n",
      " 1   time              31680 non-null  object \n",
      " 2   benefitduration2  31680 non-null  int64  \n",
      " 3   distance_fips2    31680 non-null  float64\n",
      " 4   ur_2              31680 non-null  float64\n",
      " 5   ur_2_lead1        30888 non-null  float64\n",
      " 6   JTSTSR            31680 non-null  float64\n",
      "dtypes: float64(4), int64(2), object(1)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df2_dd_lead1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Qusi-Diff df1_dd_lead1\n",
    "df1_dd_lead1.dropna(inplace=True)\n",
    "df1_dd_lead1 = df1_dd_lead1[df1_dd_lead1.ur_1 != 0]\n",
    "\n",
    "df1_dd_lead1['natural_log_ur_1'] = np.log(df1_dd_lead1['ur_1']) \n",
    "df1_dd_lead1['natural_log_ur_1_lead1'] = np.log(df1_dd_lead1['ur_1_lead1']) \n",
    "\n",
    "df1_dd_lead1['s-1'] = df1_dd_lead1['JTSTSR'].div(100).subtract(1)\n",
    "\n",
    "df1_dd_lead1['coef1'] = df1_dd_lead1['s-1'].multiply(0.9975)\n",
    "df1_dd_lead1['coef2'] = df1_dd_lead1['s-1'].multiply(0.99)\n",
    "df1_dd_lead1['coef3'] = df1_dd_lead1['s-1'].multiply(0.98)\n",
    "\n",
    "df1_dd_lead1['quasi1_1'] = df1_dd_lead1['natural_log_ur_1'] + df1_dd_lead1['coef1'] * df1_dd_lead1['natural_log_ur_1_lead1']\n",
    "df1_dd_lead1['quasi2_1'] = df1_dd_lead1['natural_log_ur_1'] + df1_dd_lead1['coef2'] * df1_dd_lead1['natural_log_ur_1_lead1']\n",
    "df1_dd_lead1['quasi3_1'] = df1_dd_lead1['natural_log_ur_1'] + df1_dd_lead1['coef3'] * df1_dd_lead1['natural_log_ur_1_lead1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/jinli/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Calculate Qusi-Diff df2_dd_lead1\n",
    "df2_dd_lead1.dropna(inplace=True)\n",
    "df2_dd_lead1 = df2_dd_lead1[df2_dd_lead1.ur_2 != 0]\n",
    "\n",
    "df2_dd_lead1['natural_log_ur_2'] = np.log(df2_dd_lead1['ur_2']) \n",
    "df2_dd_lead1['natural_log_ur_2_lead1'] = np.log(df2_dd_lead1['ur_2_lead1']) \n",
    "\n",
    "df2_dd_lead1['s-1'] = df2_dd_lead1['JTSTSR'].div(100).subtract(1)\n",
    "\n",
    "df2_dd_lead1['coef1'] = df2_dd_lead1['s-1'].multiply(0.9975)\n",
    "df2_dd_lead1['coef2'] = df2_dd_lead1['s-1'].multiply(0.99)\n",
    "df2_dd_lead1['coef3'] = df2_dd_lead1['s-1'].multiply(0.98)\n",
    "\n",
    "df2_dd_lead1['quasi1_2'] = df2_dd_lead1['natural_log_ur_2'] + df2_dd_lead1['coef1'] * df2_dd_lead1['natural_log_ur_2_lead1']\n",
    "df2_dd_lead1['quasi2_2'] = df2_dd_lead1['natural_log_ur_2'] + df2_dd_lead1['coef2'] * df2_dd_lead1['natural_log_ur_2_lead1']\n",
    "df2_dd_lead1['quasi3_2'] = df2_dd_lead1['natural_log_ur_2'] + df2_dd_lead1['coef3'] * df2_dd_lead1['natural_log_ur_2_lead1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30886 entries, 0 to 30887\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   geoid10_fips2           30886 non-null  int64  \n",
      " 1   time                    30886 non-null  object \n",
      " 2   benefitduration2        30886 non-null  int64  \n",
      " 3   distance_fips2          30886 non-null  float64\n",
      " 4   ur_2                    30886 non-null  float64\n",
      " 5   ur_2_lead1              30886 non-null  float64\n",
      " 6   JTSTSR                  30886 non-null  float64\n",
      " 7   natural_log_ur_2        30886 non-null  float64\n",
      " 8   natural_log_ur_2_lead1  30886 non-null  float64\n",
      " 9   s-1                     30886 non-null  float64\n",
      " 10  coef1                   30886 non-null  float64\n",
      " 11  coef2                   30886 non-null  float64\n",
      " 12  coef3                   30886 non-null  float64\n",
      " 13  quasi1_2                30886 non-null  float64\n",
      " 14  quasi2_2                30886 non-null  float64\n",
      " 15  quasi3_2                30886 non-null  float64\n",
      "dtypes: float64(13), int64(2), object(1)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df2_dd_lead1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46520 entries, 0 to 46519\n",
      "Data columns (total 27 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   COUNTYPAIR_ID          46520 non-null  object \n",
      " 1   Period                 46520 non-null  object \n",
      " 2   STATE_FIPS1            46520 non-null  float64\n",
      " 3   BenefitDuration1       46520 non-null  float64\n",
      " 4   GEOID10_FIPS1          46520 non-null  float64\n",
      " 5   distance_FIPS1         46520 non-null  float64\n",
      " 6   UR_1                   46520 non-null  float64\n",
      " 7   own_code_1             46520 non-null  int64  \n",
      " 8   industry_code_1        46520 non-null  int64  \n",
      " 9   agglvl_code_1          46520 non-null  int64  \n",
      " 10  size_code_1            46520 non-null  int64  \n",
      " 11  year_1                 46520 non-null  int64  \n",
      " 12  qtr_1                  46520 non-null  int64  \n",
      " 13  disclosure_code_1      2 non-null      object \n",
      " 14  area_title_1           46520 non-null  object \n",
      " 15  own_title_1            46520 non-null  object \n",
      " 16  industry_title_1       46520 non-null  object \n",
      " 17  agglvl_title_1         46520 non-null  object \n",
      " 18  size_title_1           46520 non-null  object \n",
      " 19  qtrly_estabs_count_1   46520 non-null  int64  \n",
      " 20  month1_emplvl_1        46520 non-null  int64  \n",
      " 21  month2_emplvl_1        46520 non-null  int64  \n",
      " 22  month3_emplvl_1        46520 non-null  int64  \n",
      " 23  total_qtrly_wages_1    46520 non-null  int64  \n",
      " 24  taxable_qtrly_wages_1  46520 non-null  int64  \n",
      " 25  qtrly_contributions_1  46520 non-null  int64  \n",
      " 26  avg_wkly_wage_1        46520 non-null  int64  \n",
      "dtypes: float64(5), int64(14), object(8)\n",
      "memory usage: 9.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dd_lead1.to_csv('DF1_DropDuplicates_QuasiDiff.csv', index=False)\n",
    "df2_dd_lead1.to_csv('DF2_DropDuplicates_QuasiDiff.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
