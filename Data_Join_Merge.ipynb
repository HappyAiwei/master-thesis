{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import geoplot as gplt\n",
    "import shapefile\n",
    "import osr\n",
    "import dbf\n",
    "import requests\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countypairs = pd.read_csv('/home/jinli/PycharmProjects/county-pair-list.txt')\n",
    "countypairs.drop_duplicates(subset='COUNTYPAIR_ID', inplace = True)\n",
    "\n",
    "new = countypairs['COUNTYPAIR_ID'].str.split(\"-\", n = 1, expand = True)\n",
    "\n",
    "pairid = pd.concat([new[0], new[1]], ignore_index=True)\n",
    "pairid = pairid.drop_duplicates()\n",
    "pairid = pairid.to_frame('id')\n",
    "pairid = pairid[~pairid['id'].isin(['06001', '06041', '06081','06075'])] ### county fips changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAUS data (Unemployment Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/home/jinli/Desktop/Thesis/Data/LAU(unemployment_rate)/*.csv')\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "lausdata = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "lausdata['GEOID10'] = lausdata['Series ID'].map(lambda x: x[5:10])\n",
    "lausdata['STATEFP10'] = lausdata['GEOID10'].map(lambda x: x[0:2])\n",
    "lausdata['COUNTYFP10'] = lausdata['GEOID10'].map(lambda x: x[2:])\n",
    "\n",
    "lausdata = pd.merge(pairid, lausdata, how='left', left_on='id', right_on='GEOID10')\n",
    "lausdata = lausdata.dropna()\n",
    "### a = lausdata.GEOID10.unique()\n",
    "### len(a)      1126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lausdata.to_csv('LAUS_COUNTY_MONTHLY_UNEMPLOYMENT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform monthly LAUS data to quarterly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlaus = pd.read_csv('LAUS_COUNTY_MONTHLY_UNEMPLOYMENT.csv')\n",
    "\n",
    "mlaus['id'] = mlaus['id'].astype(str)\n",
    "mlaus['id'] = mlaus['id'].str.zfill(5)\n",
    "\n",
    "mlaus['Year'] = mlaus['Year'].astype(int)\n",
    "mlaus = mlaus[(mlaus['Year'] > 2005) & (mlaus['Year'] < 2016)]\n",
    "\n",
    "mlaus['Value'] = pd.to_numeric(mlaus['Value'], errors='coerce')\n",
    "mlaus = mlaus.replace(np.nan, 0, regex=True)\n",
    "\n",
    "mlaus['Month'] = mlaus['Period'].str[1:]\n",
    "mlaus['Year-Month'] = pd.to_datetime(mlaus[['Year', 'Month']].assign(Day=1)).dt.to_period('M')\n",
    "mlaus['Qtr'] = pd.to_datetime(mlaus[['Year', 'Month']].assign(Day=1)).dt.quarter\n",
    "\n",
    "mlaus = mlaus[['id', 'Year', 'Qtr', 'Value']]\n",
    "mlaus.sort_values(by =['id', 'Year', 'Qtr'], ignore_index=True, inplace=True)\n",
    "\n",
    "qlaus = mlaus.groupby(['id', 'Year', 'Qtr'])['Value'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qlaus.to_csv('LAUS_COUNTY_QUARTERLY_UNEMPLOYMENT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoPlot: all states, all counties and broder counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All counties\n",
    "allcounties = ZipFile('/home/jinli/PycharmProjects/tl_2010_us_county10(NEW).zip', 'r')\n",
    "\n",
    "filenames_ac = [y for y in sorted(allcounties.namelist())\n",
    "                 for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)]\n",
    "\n",
    "dbf_ac, prj_ac, shp_ac, shx_ac = [io.BytesIO(allcounties.read(filename)) for filename in filenames_ac]\n",
    "\n",
    "r_ac = shapefile.Reader(shp=shp_ac, shx=shx_ac, dbf=dbf_ac)\n",
    "## r_ac.numRecords   ### 3221\n",
    "\n",
    "\n",
    "attributes, geometry = [], []\n",
    "\n",
    "field_names = [field[0] for field in r_ac.fields[1:]]\n",
    "\n",
    "for row in r_ac.shapeRecords():\n",
    "    geometry.append(shape(row.shape.__geo_interface__))\n",
    "    attributes.append(dict(zip(field_names,row.record)))\n",
    "    \n",
    "prj = io.TextIOWrapper(prj_ac, encoding='utf-8')\n",
    "proj4 = osr.SpatialReference(prj.read()).ExportToProj4()\n",
    "\n",
    "gdf_ac = gpd.GeoDataFrame(data=attributes, geometry=geometry, crs=proj4)\n",
    "gdf_ac.sort_values(by =['STATEFP10', 'COUNTYFP10'], inplace=True)\n",
    "gdf_ac.reset_index(drop=True, inplace=True)\n",
    "gdf_ac[['INTPTLON10', 'INTPTLAT10']] = gdf_ac[['INTPTLON10', 'INTPTLAT10']].apply(pd.to_numeric)\n",
    "\n",
    "gdf_ac = gdf_ac[(gdf_ac.STATEFP10 != '02') & (gdf_ac.STATEFP10 != '72') & (gdf_ac.STATEFP10 != '15')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All states\n",
    "allstates = ZipFile('/home/jinli/PycharmProjects/tl_2010_us_state10.zip', 'r')\n",
    "\n",
    "filenames_as = [y for y in sorted(allstates.namelist())\n",
    "                for ending in ['dbf', 'prj', 'shp', 'shx'] if y.endswith(ending)]\n",
    "\n",
    "dbf_as, prj_as, shp_as, shx_as = [io.BytesIO(allstates.read(filename)) for filename in filenames_as]\n",
    "\n",
    "r_as = shapefile.Reader(shp=shp_as, shx=shx_as, dbf=dbf_as)\n",
    "\n",
    "attributes, geometry = [], []\n",
    "\n",
    "field_names = [field[0] for field in r_as.fields[1:]]\n",
    "for row in r_as.shapeRecords():\n",
    "    geometry.append(shape(row.shape.__geo_interface__))\n",
    "    attributes.append(dict(zip(field_names,row.record)))\n",
    "    \n",
    "prj = io.TextIOWrapper(prj_as, encoding='utf-8')\n",
    "proj4 = osr.SpatialReference(prj.read()).ExportToProj4()\n",
    "\n",
    "gdf_as = gpd.GeoDataFrame(data=attributes, geometry=geometry, crs=proj4)\n",
    "gdf_as = gdf_as[~gdf_as['STATEFP10'].isin(['02', '72', '15'])]\n",
    "gdf_as.sort_values(by ='STATEFP10', ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_list = lausdata.GEOID10.tolist()\n",
    "cp_list = list(set(cp_list)) ## remove duplicates\n",
    "#cp_list = [e for e in cp_list if e not in ('06001', '06041', '06081','06075')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_cp = gdf_ac[gdf_ac['GEOID10'].isin(cp_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50,50))\n",
    "gdf_cp.plot(ax=ax, color='steelblue', edgecolor='none')\n",
    "gdf_ac.plot(ax=ax, facecolor='none', linewidth=0.2, edgecolor='grey')\n",
    "gdf_as.plot(ax=ax, facecolor='none', linewidth=1, edgecolor='black')\n",
    "\n",
    "fig.savefig('full_figure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QCEW (Quarterly Census of Employment and Wages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv('Paired_County_ID.csv')\n",
    "ids['id'] = ids['id'].astype(str)\n",
    "ids['id'] = ids['id'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/home/jinli/Desktop/Thesis/Data/QCEW/2005.q1-q4.by_area/*.csv')\n",
    "dfs = [pd.read_csv(file) for file in files]\n",
    "qcewdata = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcewdata05 = qcewdata[qcewdata['area_fips'].apply(lambda x: str(x).isdigit())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcewdata05['area_fips'] = qcewdata05['area_fips'].astype(str)\n",
    "qcewdata05['area_fips'] = qcewdata05['area_fips'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcewdata05 = pd.merge(qcewdata05, ids, how='left', left_on='area_fips', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01001 = pd.read_csv('/home/jinli/Desktop/Thesis/Data/QCEW/2005.q1-q4.by_area/2005.q1-q4 01001 Autauga County, Alabama.csv')\n",
    "df01001['area_fips'] = df01001['area_fips'].astype(str)\n",
    "df01001['area_fips'] = df01001['area_fips'].str.zfill(5)\n",
    "df01001.drop(df01001.columns[21:], axis=1, inplace=True)\n",
    "df01001 = df01001.loc[df01001['own_code']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Benefit Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbe = pd.read_excel('MaxBenefitExtension.xlsx', index_col=0) \n",
    "\n",
    "mbe.drop(columns=['2016Q1', '2016Q2', '2016Q3', '2016Q4'], inplace=True)\n",
    "mbe.reset_index(inplace=True)\n",
    "mbe = mbe.iloc[:, 2:]\n",
    "\n",
    "mbe = mbe.melt(id_vars=['state_id'], ignore_index=True)\n",
    "mbe.fillna(0, inplace=True) \n",
    "mbe['value'] += 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbe.to_csv('Maximum_Benefit_Duration.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:  county-pair ids and centroid distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_dist = pd.read_csv('CountyPair_Centroid_Border_Distance.csv') ### 1178 entries\n",
    "cp_id = pd.read_csv('Paired_County_ID.csv') ### 1126 entries\n",
    "\n",
    "cp_dist = pd.merge(cp_dist, cp_id, how='left', left_on='GEOID10_FIPS1', right_on='id')  \n",
    "cp_dist = pd.merge(cp_dist, cp_id, how='left', left_on='GEOID10_FIPS2', right_on='id')\n",
    "#7   id_x            1166 non-null   float64\n",
    "#8   id_y            1172 non-null   float64\n",
    "cp_dist.dropna(inplace=True)  ### 1163 rows × 9 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: cp_dist and benefit extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbe = pd.read_csv('Maximum_Benefit_Duration.csv')\n",
    "\n",
    "df = pd.merge(cp_dist, mbe, how='outer', left_on='STATE_FIPS1', right_on='state_id') \n",
    "# cp_dist.shape (1163, 9)\n",
    "# mbe.shape (2120, 3)   53*40=2120\n",
    "# df.shape (46680, 12)  1163*40=46520\n",
    "#---  ------          --------------  -----\n",
    "# 1   STATE_FIPS1     46520 non-null  float64\n",
    "# 9   state_id        46680 non-null  int64\n",
    "df.dropna(inplace=True)\n",
    "df.drop(df.columns[7:10], axis=1, inplace=True)\n",
    "df.rename({'variable': 'Period', 'value': 'BenefitDuration1'}, axis=1, inplace=True)\n",
    "df = df[['COUNTYPAIR_ID', 'Period', \n",
    "         'STATE_FIPS1', 'BenefitDuration1', 'GEOID10_FIPS1', 'distance_FIPS1', \n",
    "         'STATE_FIPS2', 'GEOID10_FIPS2', 'distance_FIPS2']]\n",
    "\n",
    "\n",
    "df = pd.merge(df, mbe, left_on=['STATE_FIPS2', 'Period'], right_on=['state_id', 'variable'])\n",
    "df.drop(['variable'], axis=1, inplace=True)\n",
    "df.rename({'value': 'BenefitDuration2'}, axis=1, inplace=True)\n",
    "df = df[['COUNTYPAIR_ID', 'Period', \n",
    "         'STATE_FIPS1', 'BenefitDuration1', 'GEOID10_FIPS1', 'distance_FIPS1', \n",
    "         'STATE_FIPS2', 'BenefitDuration2', 'GEOID10_FIPS2', 'distance_FIPS2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: merging quarterly unemployment rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ur = pd.read_csv('LAUS_COUNTY_QUARTERLY_UNEMPLOYMENT.csv')\n",
    "\n",
    "ur['Period'] = ur.Year.map(str) + 'Q' + ur.Qtr.map(str)\n",
    "ur = ur[['id', 'Period', 'Value']]\n",
    "ur.rename({'Value': 'UR'}, axis=1, inplace=True)\n",
    "\n",
    "df1 = df[['COUNTYPAIR_ID', 'Period', 'STATE_FIPS1', 'BenefitDuration1', 'GEOID10_FIPS1', 'distance_FIPS1']]\n",
    "df2 = df[['COUNTYPAIR_ID', 'Period', 'STATE_FIPS2', 'BenefitDuration2', 'GEOID10_FIPS2', 'distance_FIPS2']]\n",
    "\n",
    "df1 = pd.merge(df1, ur, left_on=['GEOID10_FIPS1', 'Period'], right_on=['id', 'Period'])\n",
    "df2 = pd.merge(df2, ur, left_on=['GEOID10_FIPS2', 'Period'], right_on=['id', 'Period'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: merging QCEW data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcew = pd.read_csv('QCEW_QUARTERLY_CENSUS_of_EMPLOYMENT_and_WAGES.csv')\n",
    "\n",
    "qcew['Period'] = qcew.year.map(str) + 'Q' + qcew.qtr.map(str)\n",
    "\n",
    "df1 = pd.merge(df1, qcew, left_on=['GEOID10_FIPS1', 'Period'], right_on=['area_fips', 'Period'])\n",
    "df2 = pd.merge(df2, qcew, left_on=['GEOID10_FIPS2', 'Period'], right_on=['area_fips', 'Period'])\n",
    "\n",
    "df1.drop(['id_x', 'area_fips', 'id_y'], axis=1, inplace=True)\n",
    "df1_1 = df1.iloc[:, 0:6]\n",
    "df1_2 = df1.iloc[:, 6:]\n",
    "df1_2 = df1_2.add_suffix('_1')\n",
    "df1 = pd.concat([df1_1, df1_2], axis=1)\n",
    "\n",
    "df2.drop(['id_x', 'area_fips', 'id_y'], axis=1, inplace=True)\n",
    "df2_1 = df2.iloc[:, 0:6]\n",
    "df2_2 = df2.iloc[:, 6:]\n",
    "df2_2 = df2_2.add_suffix('_2')\n",
    "df2 = pd.concat([df2_1, df2_2], axis=1)\n",
    "\n",
    "df1.sort_values(by =['COUNTYPAIR_ID', 'Period'], inplace=True)\n",
    "df2.sort_values(by =['COUNTYPAIR_ID', 'Period'], inplace=True)\n",
    "\n",
    "#df1.to_csv('DataFrame1.csv', index=False)\n",
    "#df2.to_csv('DataFrame2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: drop duplicates to generate pannel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dd = df1.drop(['COUNTYPAIR_ID'], axis=1)\n",
    "\n",
    "first_col = ['GEOID10_FIPS1']\n",
    "last_cols = [col for col in df1_dd.columns if col not in first_col]\n",
    "\n",
    "df1_dd = df1_dd[first_col + last_cols]\n",
    "\n",
    "df1_dd = df1_dd.drop_duplicates(subset=['GEOID10_FIPS1', 'Period'])\n",
    "\n",
    "df1_dd.to_csv('DataFrame1_Drop_Duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_dd = df2.drop(['COUNTYPAIR_ID'], axis=1)\n",
    "\n",
    "first_col = ['GEOID10_FIPS2']\n",
    "last_cols = [col for col in df2_dd.columns if col not in first_col]\n",
    "\n",
    "df2_dd = df2_dd[first_col + last_cols]\n",
    "\n",
    "df2_dd = df2_dd.drop_duplicates(subset=['GEOID10_FIPS2', 'Period'])\n",
    "\n",
    "df2_dd.to_csv('DataFrame2_Drop_Duplicates.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: generate county-pair-data with 1 lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read total separation rates\n",
    "tsr = pd.read_csv('TotalSeparationRates_Nonfarm_QuarterlySum .csv')\n",
    "tsr['time'] = pd.PeriodIndex(tsr.DATE, freq='Q')\n",
    "tsr = tsr[['DATE', 'time', 'JTSTSR']]\n",
    "# period[Q-DEC] column to string\n",
    "tsr.time = tsr.time.astype(str)\n",
    "tsr['time'] = tsr['time'].str.replace('Q', 'q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('DataFrame1.csv')\n",
    "df2 = pd.read_csv('DataFrame2.csv')\n",
    "\n",
    "df1_dd_lead1 = pd.read_csv('DataFrame1_Drop_Duplicates_Lead1.csv')\n",
    "df2_dd_lead1 = pd.read_csv('DataFrame2_Drop_Duplicates_Lead1.csv')\n",
    "\n",
    "df1_dd_lead1 = pd.merge(df1_dd_lead1, tsr, on='time', validate='many_to_one')\n",
    "df2_dd_lead1 = pd.merge(df2_dd_lead1, tsr, on='time', validate='many_to_one')\n",
    "\n",
    "df1_dd_lead1.drop(df1_dd_lead1.iloc[:, 6:28], axis=1, inplace=True)\n",
    "df2_dd_lead1.drop(df2_dd_lead1.iloc[:, 6:28], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_dd_lead1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Qusi-Diff df1_dd_lead1\n",
    "df1_dd_lead1.dropna(inplace=True)\n",
    "df1_dd_lead1 = df1_dd_lead1[df1_dd_lead1.ur_1 != 0]\n",
    "\n",
    "df1_dd_lead1['natural_log_ur_1'] = np.log(df1_dd_lead1['ur_1']) \n",
    "df1_dd_lead1['natural_log_ur_1_lead1'] = np.log(df1_dd_lead1['ur_1_lead1']) \n",
    "\n",
    "df1_dd_lead1['s-1'] = df1_dd_lead1['JTSTSR'].div(100).subtract(1)\n",
    "\n",
    "df1_dd_lead1['coef1'] = df1_dd_lead1['s-1'].multiply(0.9975)\n",
    "df1_dd_lead1['coef2'] = df1_dd_lead1['s-1'].multiply(0.99)\n",
    "df1_dd_lead1['coef3'] = df1_dd_lead1['s-1'].multiply(0.98)\n",
    "\n",
    "df1_dd_lead1['quasi1_1'] = df1_dd_lead1['natural_log_ur_1'] + df1_dd_lead1['coef1'] * df1_dd_lead1['natural_log_ur_1_lead1']\n",
    "df1_dd_lead1['quasi2_1'] = df1_dd_lead1['natural_log_ur_1'] + df1_dd_lead1['coef2'] * df1_dd_lead1['natural_log_ur_1_lead1']\n",
    "df1_dd_lead1['quasi3_1'] = df1_dd_lead1['natural_log_ur_1'] + df1_dd_lead1['coef3'] * df1_dd_lead1['natural_log_ur_1_lead1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Qusi-Diff df2_dd_lead1\n",
    "df2_dd_lead1.dropna(inplace=True)\n",
    "df2_dd_lead1 = df2_dd_lead1[df2_dd_lead1.ur_2 != 0]\n",
    "\n",
    "df2_dd_lead1['natural_log_ur_2'] = np.log(df2_dd_lead1['ur_2']) \n",
    "df2_dd_lead1['natural_log_ur_2_lead1'] = np.log(df2_dd_lead1['ur_2_lead1']) \n",
    "\n",
    "df2_dd_lead1['s-1'] = df2_dd_lead1['JTSTSR'].div(100).subtract(1)\n",
    "\n",
    "df2_dd_lead1['coef1'] = df2_dd_lead1['s-1'].multiply(0.9975)\n",
    "df2_dd_lead1['coef2'] = df2_dd_lead1['s-1'].multiply(0.99)\n",
    "df2_dd_lead1['coef3'] = df2_dd_lead1['s-1'].multiply(0.98)\n",
    "\n",
    "df2_dd_lead1['quasi1_2'] = df2_dd_lead1['natural_log_ur_2'] + df2_dd_lead1['coef1'] * df2_dd_lead1['natural_log_ur_2_lead1']\n",
    "df2_dd_lead1['quasi2_2'] = df2_dd_lead1['natural_log_ur_2'] + df2_dd_lead1['coef2'] * df2_dd_lead1['natural_log_ur_2_lead1']\n",
    "df2_dd_lead1['quasi3_2'] = df2_dd_lead1['natural_log_ur_2'] + df2_dd_lead1['coef3'] * df2_dd_lead1['natural_log_ur_2_lead1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_dd_lead1.to_csv('DF1_DropDuplicates_QuasiDiff.csv', index=False)\n",
    "df2_dd_lead1.to_csv('DF2_DropDuplicates_QuasiDiff.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: merge all data with county-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinli/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('DataFrame1.csv')\n",
    "df2 = pd.read_csv('DataFrame2.csv')\n",
    "df1_quasi = pd.read_csv('DF1_DropDuplicates_QuasiDiff.csv')\n",
    "df2_quasi = pd.read_csv('DF2_DropDuplicates_QuasiDiff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['GEOID10_FIPS1'] = df1['GEOID10_FIPS1'].astype(int)\n",
    "\n",
    "df1.Period = df1.Period.astype(str)\n",
    "df1.Period = df1.Period.str.replace('Q', 'q')\n",
    "\n",
    "df1 = pd.merge(df1, df1_quasi, left_on=['GEOID10_FIPS1', 'Period'], \n",
    "               right_on=['geoid10_fips1', 'time'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['GEOID10_FIPS2'] = df2['GEOID10_FIPS2'].astype(int)\n",
    "\n",
    "df2.Period = df2.Period.astype(str)\n",
    "df2.Period = df2.Period.str.replace('Q', 'q')\n",
    "\n",
    "df2 = pd.merge(df2, df2_quasi, left_on=['GEOID10_FIPS2', 'Period'], \n",
    "               right_on=['geoid10_fips2', 'time'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.merge(df1, df2, on=['COUNTYPAIR_ID', 'Period'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_df = concatenated_df[['COUNTYPAIR_ID','Period',\n",
    "                             'GEOID10_FIPS1','distance_FIPS1','BenefitDuration1',\n",
    "                             'UR_1','ur_1','ur_1_lead1','natural_log_ur_1','natural_log_ur_1_lead1',\n",
    "                             'JTSTSR_x','coef1_x','coef2_x','coef3_x','quasi1_1','quasi2_1','quasi3_1',\n",
    "                             'qtrly_estabs_count_1','month1_emplvl_1','month2_emplvl_1','month3_emplvl_1',\n",
    "                             'total_qtrly_wages_1','taxable_qtrly_wages_1','qtrly_contributions_1','avg_wkly_wage_1',\n",
    "                             'GEOID10_FIPS2','distance_FIPS2','BenefitDuration2',\n",
    "                             'UR_2','ur_2','ur_2_lead1','natural_log_ur_2','natural_log_ur_2_lead1',\n",
    "                             'JTSTSR_y','coef1_y','coef2_y','coef3_y','quasi1_2','quasi2_2','quasi3_2',\n",
    "                             'qtrly_estabs_count_2','month1_emplvl_2','month2_emplvl_2','month3_emplvl_2',\n",
    "                             'total_qtrly_wages_2','taxable_qtrly_wages_2','qtrly_contributions_2','avg_wkly_wage_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinli/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4164: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "paired_df.drop(['JTSTSR_y','coef1_y','coef2_y','coef3_y'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COUNTYPAIR_ID',\n",
       " 'Period',\n",
       " 'GEOID10_FIPS1',\n",
       " 'distance_FIPS1',\n",
       " 'BenefitDuration1',\n",
       " 'UR_1',\n",
       " 'ur_1',\n",
       " 'ur_1_lead1',\n",
       " 'natural_log_ur_1',\n",
       " 'natural_log_ur_1_lead1',\n",
       " 'JTSTSR_x',\n",
       " 'coef1_x',\n",
       " 'coef2_x',\n",
       " 'coef3_x',\n",
       " 'quasi1_1',\n",
       " 'quasi2_1',\n",
       " 'quasi3_1',\n",
       " 'qtrly_estabs_count_1',\n",
       " 'month1_emplvl_1',\n",
       " 'month2_emplvl_1',\n",
       " 'month3_emplvl_1',\n",
       " 'total_qtrly_wages_1',\n",
       " 'taxable_qtrly_wages_1',\n",
       " 'qtrly_contributions_1',\n",
       " 'avg_wkly_wage_1',\n",
       " 'GEOID10_FIPS2',\n",
       " 'distance_FIPS2',\n",
       " 'BenefitDuration2',\n",
       " 'UR_2',\n",
       " 'ur_2',\n",
       " 'ur_2_lead1',\n",
       " 'natural_log_ur_2',\n",
       " 'natural_log_ur_2_lead1',\n",
       " 'quasi1_2',\n",
       " 'quasi2_2',\n",
       " 'quasi3_2',\n",
       " 'qtrly_estabs_count_2',\n",
       " 'month1_emplvl_2',\n",
       " 'month2_emplvl_2',\n",
       " 'month3_emplvl_2',\n",
       " 'total_qtrly_wages_2',\n",
       " 'taxable_qtrly_wages_2',\n",
       " 'qtrly_contributions_2',\n",
       " 'avg_wkly_wage_2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(paired_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinli/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "paired_df = paired_df.dropna() \n",
    "\n",
    "paired_df['natural_log_BenefitDuration1'] = np.log(paired_df['BenefitDuration1']) \n",
    "paired_df['natural_log_BenefitDuration2'] = np.log(paired_df['BenefitDuration2']) \n",
    "\n",
    "paired_df['natural_log_distance_FIPS1'] = np.log(paired_df['distance_FIPS1']) \n",
    "paired_df['natural_log_distance_FIPS2'] = np.log(paired_df['distance_FIPS2']) \n",
    "\n",
    "paired_df['natural_log_avg_wkly_wage_1'] = np.log(paired_df['avg_wkly_wage_1']) \n",
    "paired_df['natural_log_avg_wkly_wage_2'] = np.log(paired_df['avg_wkly_wage_2'])\n",
    "\n",
    "paired_df['delta_quasi1'] = paired_df['quasi1_1'] - paired_df['quasi1_2']\n",
    "paired_df['delta_quasi2'] = paired_df['quasi2_1'] - paired_df['quasi2_2']\n",
    "paired_df['delta_quasi3'] = paired_df['quasi3_1'] - paired_df['quasi3_2']\n",
    "\n",
    "paired_df['delta_benefit'] = paired_df['natural_log_BenefitDuration1'] - paired_df['natural_log_BenefitDuration2']\n",
    "\n",
    "paired_df['delta_distance'] = paired_df['natural_log_distance_FIPS1'] - paired_df['natural_log_distance_FIPS2']\n",
    "\n",
    "paired_df['delta_wkly_wage'] = paired_df['natural_log_avg_wkly_wage_1'] - paired_df['natural_log_avg_wkly_wage_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COUNTYPAIR_ID',\n",
       " 'Period',\n",
       " 'GEOID10_FIPS1',\n",
       " 'distance_FIPS1',\n",
       " 'BenefitDuration1',\n",
       " 'UR_1',\n",
       " 'ur_1',\n",
       " 'ur_1_lead1',\n",
       " 'natural_log_ur_1',\n",
       " 'natural_log_ur_1_lead1',\n",
       " 'JTSTSR_x',\n",
       " 'coef1_x',\n",
       " 'coef2_x',\n",
       " 'coef3_x',\n",
       " 'quasi1_1',\n",
       " 'quasi2_1',\n",
       " 'quasi3_1',\n",
       " 'qtrly_estabs_count_1',\n",
       " 'month1_emplvl_1',\n",
       " 'month2_emplvl_1',\n",
       " 'month3_emplvl_1',\n",
       " 'total_qtrly_wages_1',\n",
       " 'taxable_qtrly_wages_1',\n",
       " 'qtrly_contributions_1',\n",
       " 'avg_wkly_wage_1',\n",
       " 'GEOID10_FIPS2',\n",
       " 'distance_FIPS2',\n",
       " 'BenefitDuration2',\n",
       " 'UR_2',\n",
       " 'ur_2',\n",
       " 'ur_2_lead1',\n",
       " 'natural_log_ur_2',\n",
       " 'natural_log_ur_2_lead1',\n",
       " 'quasi1_2',\n",
       " 'quasi2_2',\n",
       " 'quasi3_2',\n",
       " 'qtrly_estabs_count_2',\n",
       " 'month1_emplvl_2',\n",
       " 'month2_emplvl_2',\n",
       " 'month3_emplvl_2',\n",
       " 'total_qtrly_wages_2',\n",
       " 'taxable_qtrly_wages_2',\n",
       " 'qtrly_contributions_2',\n",
       " 'avg_wkly_wage_2',\n",
       " 'natural_log_BenefitDuration1',\n",
       " 'natural_log_BenefitDuration2',\n",
       " 'natural_log_distance_FIPS1',\n",
       " 'natural_log_distance_FIPS2',\n",
       " 'natural_log_avg_wkly_wage_1',\n",
       " 'natural_log_avg_wkly_wage_2',\n",
       " 'delta_quasi1',\n",
       " 'delta_quasi2',\n",
       " 'delta_quasi3',\n",
       " 'delta_benefit',\n",
       " 'delta_distance',\n",
       " 'delta_wkly_wage']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(paired_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = paired_df[['COUNTYPAIR_ID', 'Period', 'delta_quasi1', 'delta_quasi2', 'delta_quasi3',\n",
    "                    'delta_benefit', 'delta_distance', 'delta_wkly_wage',\n",
    "                    'natural_log_ur_1', 'natural_log_ur_1_lead1', 'natural_log_ur_2', 'natural_log_ur_2_lead1',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45351 entries, 0 to 46518\n",
      "Data columns (total 12 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   COUNTYPAIR_ID           45351 non-null  object \n",
      " 1   Period                  45351 non-null  object \n",
      " 2   delta_quasi1            45351 non-null  float64\n",
      " 3   delta_quasi2            45351 non-null  float64\n",
      " 4   delta_quasi3            45351 non-null  float64\n",
      " 5   delta_benefit           45351 non-null  float64\n",
      " 6   delta_distance          45351 non-null  float64\n",
      " 7   delta_wkly_wage         45351 non-null  float64\n",
      " 8   natural_log_ur_1        45351 non-null  float64\n",
      " 9   natural_log_ur_1_lead1  45351 non-null  float64\n",
      " 10  natural_log_ur_2        45351 non-null  float64\n",
      " 11  natural_log_ur_2_lead1  45351 non-null  float64\n",
      "dtypes: float64(10), object(2)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_reg.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = df_reg[df_reg.delta_benefit == 0] ## 32005 entries\n",
    "# b = df_reg[df_reg.delta_distance == 0] ## 0 entries\n",
    "# c = df_reg[df_reg.delta_wkly_wage == 0] ## 150 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg1 = df_reg[df_reg.delta_benefit != 0] ## 13346 entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg2 = df_reg[(df_reg.delta_benefit != 0) & (df_reg.delta_wkly_wage != 0)] ## 13299 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg3 = df_reg[(df_reg.delta_benefit != 0) & (df_reg.delta_wkly_wage != 0) &(df_reg.delta_distance != 0)] \n",
    "## 13299 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping unique value to each county-pair-id\n",
    "df_reg1  = df_reg1.assign(id = df_reg1.COUNTYPAIR_ID.astype('category').cat.codes)\n",
    "\n",
    "df_reg1 = df_reg1[['id', 'COUNTYPAIR_ID', 'Period', 'delta_quasi1', 'delta_quasi2', 'delta_quasi3',\n",
    "                   'delta_benefit', 'delta_distance', 'delta_wkly_wage',\n",
    "                   'natural_log_ur_1', 'natural_log_ur_1_lead1', 'natural_log_ur_2', 'natural_log_ur_2_lead1']]\n",
    "\n",
    "#df_reg1.to_csv('DataFrame_to_Regress_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg2  = df_reg2.assign(id = df_reg2.COUNTYPAIR_ID.astype('category').cat.codes)\n",
    "\n",
    "df_reg2 = df_reg2[['id', 'COUNTYPAIR_ID', 'Period', 'delta_quasi1', 'delta_quasi2', 'delta_quasi3',\n",
    "                   'delta_benefit', 'delta_distance', 'delta_wkly_wage',\n",
    "                   'natural_log_ur_1', 'natural_log_ur_1_lead1', 'natural_log_ur_2', 'natural_log_ur_2_lead1']]\n",
    "\n",
    "#df_reg2.to_csv('DataFrame_to_Regress_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
